---
title: "Kalman Filter and Smoother Derivations"
format: html
editor: visual
bibliography: bibliography.bib
---

We will use the notation and follow chapter 6 of [@ShumwayStoffer].

## General Linear Gaussian State Space

$$
\begin{align}
x_{t} &= \Phi_t x_{t-1} + \Upsilon_t u_t + w_t \\
y_t &= A_t x_t + \Gamma_t u_t + v_t \\
w_t & \sim N(0, Q_t) \quad \text{i.i.d} \\
v_t & \sim N(0, R_t) \quad \text{i.i.d} \\
w_{t_i} & \perp v_{t_j} \quad \forall \quad t_i, t_j
\end{align}
$$ {#eq-state-space}

where

-   Vectors

    -   $x_t$ : state vector (p x 1)

    -   $y_t$ : observation vector (q x 1)

    -   $u_t$ : exogenous input vector (r x 1)

## Notation

$$
x^{s}_t = E(x_t|y_{1:s})
$$ {#eq-conditional-mean}

$$
P^{s}_{t1,t2} = E\{(x_{t_1} - x^s_{t_1})(x_{t_2} - x^s_{t_2})^\top|y_{1:s}\}
$$ {#eq-conditional-covariance}

where for ease of notation $P^{s}_{t} := P^{s}_{t,t}$.

## Kalman Filter

Assuming known initial conditions $x^0_0$ and $P^0_0$ we have

### Filtering

$$
x^{t-1}_t = \Phi_t x^{t-1}_{t-1} + \Upsilon_t u_t 
$$ {#eq-filter-state}

$$
P^{t-1}_t = \Phi_t P^{t-1}_{t-1} \Phi^{\top}_{t} + Q_t
$$ {#eq-filter-covariance}

### Prediction

$$ x^{t}_t = x^{t-1}_{t} + K_t (y_t - A_t x^{t-1}_{t} - \Gamma u_t) $$ {#eq-predicted-state}

$$ P^{t}_t = (I - K_t A_t)P^{t-1}_t $$ {#eq-predicted-covariance}

Where the Kalman gain is

$$
K_t = P^{t-1}_t A^\top_t(A_t P^{t-1}_t A^{\top}_t + R_t)^{-1}
$$

Innovations and their covariance are then defined as

$$
\epsilon_t = y_t - E(y_t|y_{1:(t-1)}) = y_t - A_t x^{t-1}_t - \Gamma u_t
$$ {#eq-innovations}

$$
\Gamma_t = \text{var}(\epsilon_t) = \text{var}\{A_t(x_t - x^{t-1}_t) + v_t\} = A_t P^{t-1}_t A^\top_t + R_t
$$ {#eq-innovations-covariance}

## Kalman Smoother

$$
x^T_{t-1} = x^{t-1}_{t-1} + J_{t-1} (x^T_{t} - x^{t-1}_t)
$$ {#eq-smoothed-state}

$$
P^T_{t-1} = P^{t-1}_{t-1} + J_{t-1}(P^T_{t} - P^{t-1}_t) J^\top_{t-1}
$$ {#eq-smoothed-covariance}

where

$$
J_{t-1} = P^{t-1}_{t-1} \Phi_t (P^{t-1}_{t})^{-1}
$$ {#eq-smoother-gain}

### Lag-One Covariance Smoother

$$
P^T_{t-1, t-2} = P^{t-1}_{t-1} J^\top_{t-2} + J_{t-1}(P^T_{t,t-1} - \Phi_t P^{t-1}_{t-1}) J^\top_{t-2}
$$ {#eq-lagone-covariance-smoother-recursion}

with initial condition

$$
P^T_{n-1, n-2} = (I - K_n A_n) \Phi_n P^{n-1}_{n-1}
$$ {#eq-lagone-covariance-smoother-initialization}

## Likelihood

### Complete Data Likelihood

The two times the negative "complete data" likelihood is: $$
\begin{aligned}
-2 \ln L_{X,Y}(\Theta) &= \ln |\Sigma_0| + (x_0 - \mu_0)^\top \Sigma_0^{-1} (x_0 - \mu_0) \\
&\quad + \sum^{T}_{t=1}\ln |Q_t| + \sum_{t=1}^{T} (x_t - \Phi x_{t-1} - \Upsilon_t u_t)^\top Q^{-1}_t (x_t - \Phi x_{t-1} - \Upsilon_t u_t) \\
&\quad + \sum^{T}_{t=1}\ln |R_t| + \sum_{t=1}^{T} (y_t - A_t x_t -  \Gamma_t u_t)^\top R^{-1}_t (y_t - A_t x_t -  \Gamma_t u_t)
\end{aligned}
$$ {#eq-complete-nllik}

### Marginal Likelihood

Defining with $\Theta$ the vector of all unknown parameters, using \\ref{#eq-innovations} and \\ref{#eq-innovations-covariance} as a function of $\Theta$ the two times negative marginal likelihood for $y$ is:

$$
-2 \log L_Y(\Theta) = \sum^{T}_{t = 1}\log |\Sigma_t(\Theta)| + \sum^{T}_{t=1} \epsilon^\top_t (\Theta) \Sigma^{-1}_t(\Theta) \epsilon_t (\Theta)
$$ {#eq-marginal-nllik}

## Constant terms model

In this application we consider a simpler model without time varying parameters and external inputs only in the observation equation

$$
\begin{align}
x_{t} &= \Phi x_{t-1} + w_t \\
y_t &= A x_t + \Gamma_t u + v_t
\end{align}
$$ {#eq-state-space-time-fixed}

The exception is that $\Gamma_t$ is time varying but considered known while $u$ is fixed but unknown.

## Expectation Maximization
