---
title: "Kalman Filter and Smoother Derivations"
format: html
editor: visual
bibliography: bibliography.bib
---

We will use the notation and follow chapter 6 of [@ShumwayStoffer].

## General Linear Gaussian State Space

$$
\begin{align}
x_{t} &= \Phi_t x_{t-1} + \Upsilon_t u_t + w_t \\
y_t &= A_t x_t + \Gamma_t u_t + v_t
\end{align}
$$ {#eq-state-space}

where

-   Vectors

    -   $x_t$ : state vector (p x 1)

    -   $y_t$ : observation vector (q x 1)

    -   $u_t$ : exogenous input vector (r x 1)

-   Erros

    -   $w_t \sim N(0, Q_t)$ i.i.d

    -   $v_t \sim N(0, R_t)$ i.i.d

## Notation

$$
x^{s}_t = E(x_t|y_{1:s})
$$ {#eq-conditional-mean}

$$
P^{s}_{t1,t2} = E\{(x_{t_1} - x^s_{t_1})(x_{t_2} - x^s_{t_2})^\top|y_{1:s}\}
$$ {#eq-conditional-covariance}

## Kalman Filter

Assuming known initial conditions $x^0_0$ and $P^0_0$ we have

### Filtering

$$
x^{t-1}_t = \Phi_t x^{t-1}_{t-1} + \Upsilon_t u_t 
$$ {#eq-filter-state}

$$
P^{t-1}_t = \Phi_t P^{t-1}_{t-1} \Phi^{\top}_{t} + Q_t
$$ {#eq-filter-covariance}

### Prediction

$$ x^{t}_t = x^{t-1}_{t} + K_t (y_t - A_t x^{t-1}_{t} - \Gamma u_t) $$ {#eq-predicted-state}

$$ P^{t}_t = (I - K_t A_t)P^{t-1}_t $$ {#eq-predicted-covariance}

Where the Kalman gain is

$$
K_t = P^{t-1}_t A^\top_t(A_t P^{t-1}_t A^{\top}_t + R_t)^{-1}
$$

Innovations and their covariance are then defined as

$$
\epsilon_t = y_t - E(y_t|y_{1:(t-1)}) = y_t - A_t x^{t-1}_t - \Gamma u_t
$$ {#eq-innovations}

$$
\Gamma_t = \text{var}(\epsilon_t) = \text{var}\{A_t(x_t - x^{t-1}_t) + v_t\} = A_t P^{t-1}_t A^\top_t
$$ {#eq-innovations-covariance}

## Kalman Smoother

$$
x^T_{t-1} = x^{t-1}_{t-1} + J_{t-1} (x^T_{t} - x^{t-1}_t)
$$ {#eq-smoothed-state}

$$
P^T_{t-1} = P^{t-1}_{t-1} + J_{t-1}(P^T_{t} - P^{t-1}_t) J^\top_{t-1}
$$ {#eq-smoothed-covariance}

where

$$
J_{t-1} = P^{t-1}_{t-1} \Phi_t (P^{t-1}_{t})^{-1}
$$ {#eq-smoother-gain}

### Lag-One Covariance Smoother

$$
P^T_{t-1, t-2} = P^{t-1}_{t-1} J^\top_{t-2} + J_{t-1}(P^T_{t,t-1} - \Phi_t P^{t-1}_{t-1}) J^\top_{t-2}
$$ {#eq-lagone-covariance-smoother-recursion}

with initial condition

$$
P^T_{n-1, n-2} = (I - K_n A_n) \Phi_n P^{n-1}_{n-1}
$$ {#eq-lagone-covariance-smoother-initialization}

## Complete Data Likelihood

The "complete data" likelihood including the states is without considering the external inputs

$$
\begin{align}Q\left( \Theta \mid \Theta^{(j-1)} \right) = \ln \left| \Sigma_0 \right| + \text{tr} \left\{ \Sigma_0^{-1} \left[ P_0^T + \left( x_0^T - \mu_0 \right) \left( x_0^T - \mu_0 \right)^\top \right] \right\} \\
+ \sum^T_{t=1} \ln \left| Q_t \right| + \text{tr} \left\{ Q^{-1} \left[ S_{11} - S_{10} \Phi^\top - \Phi S_{10}^\top + \Phi S_{00} \Phi^\top \right] \right\} \\
+ \sum^T_{t=1} \ln \left| R_t \right| + \text{tr} \left\{ R^{-1} \sum_{t=1}^{n} \left[ \left( y_t - A_t x_t^T \right) \left( y_t - A_t x_t^T \right)^\top + A_t P_t^T A_t^\top \right] \right\}\end{align}
$$ {#eq-complete_llik}

## Constant terms model

In this application we consider a simpler model without time varying parameters and external inputs only in the observation equation

$$
\begin{align}
x_{t} &= \Phi x_{t-1} + w_t \\
y_t &= A x_t + \Gamma_t u + v_t
\end{align}
$$ {#eq-state-space-time-fixed}

The exception is that $\Gamma_t$ is time varying but considered known while $u$ is fixed but unknown.
